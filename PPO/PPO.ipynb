{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "LEARNING_RATE = 0.0003\n",
    "GAMMA = 0.99\n",
    "EPSILON_CLIP = 0.2\n",
    "ENTROPY_COEFF = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "TIMESTEPS = 2048\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Critic Network\n",
    "\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "    \n",
    "        # actor\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_action_and_value(self, state):\n",
    "\n",
    "        action_probs = self.actor(state)  # pi(a|s) => left|right => ex : [0.7, 0.3]\n",
    "        state_values = self.critic(state) # v(s), R + rV(s') - \"V(s)\"\n",
    "\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample() # left|right = [0.75, 0.25] => 0번 선택\n",
    "        action_logprobs = dist.log_prob(action) # log(0.75)\n",
    "        entropy = dist.entropy() # entropy([0.75, 0.25])\n",
    "\n",
    "        return action, action_logprobs, state_values, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.dones = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(buffer, old_model, new_model, optimizer):\n",
    "    # calculate last state_value for : r + gamma * V(t)\n",
    "    state = buffer.states[-1]\n",
    "    done = buffer.dones[-1]\n",
    "    with torch.no_grad():\n",
    "        discounted_rewards = 0 if done else old_model.get_action_and_value(torch.FloatTensor(state))\n",
    "\n",
    "    returns = []\n",
    "    for reward in reversed(buffer.rewards):\n",
    "        discounted_rewards = reward + GAMMA * discounted_rewards\n",
    "        returns.insert(0, discounted_rewards)\n",
    "\n",
    "    advantages = torch.FloatTensor(returns) - torch.FloatTensor(buffer.state_values)\n",
    "\n",
    "    for _ in range(EPOCHS):\n",
    "        for idx in range(0, len(buffer.states), BATCH_SIZE):\n",
    "            batch_states = torch.FloatTensor(buffer.states[idx : idx + BATCH_SIZE])\n",
    "            batch_actions = torch.LongTensor(buffer.actions[idx : idx + BATCH_SIZE])\n",
    "\n",
    "            batch_returns = torch.FloatTensor(returns[idx : idx + BATCH_SIZE])\n",
    "            batch_advantages = torch.FloatTensor(advantages[idx : idx + BATCH_SIZE])\n",
    "\n",
    "            # new_model에서 새로운 정책 계산\n",
    "            new_policy_logits = new_model.actor(batch_states)\n",
    "            values = new_model.critic(batch_states)\n",
    "            new_policy_dist = Categorical(logits = new_policy_logits)\n",
    "            new_log_probs = new_policy_dist.log_prob(batch_actions)\n",
    "            entropy = new_policy_dist.entropy()\n",
    "\n",
    "            # old_model에서 이전 정책 계산\n",
    "            with torch.no_grad():\n",
    "                old_policy_logits = old_model.actor(batch_states)\n",
    "                old_policy_dist = Categorical(logits = old_policy_logits)\n",
    "                old_log_probs = old_policy_dist.log_prob(batch_actions)\n",
    "\n",
    "            # Compute ratio\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            # PPO Loss\n",
    "            surrogate1 = ratios * batch_advantages\n",
    "            surrogate2 = torch.clamp(ratios, 1 - EPSILON_CLIP, 1 + EPSILON_CLIP) * batch_advantages\n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), batch_returns)\n",
    "\n",
    "            entropy_loss = - ENTROPY_COEFF * entropy.mean()\n",
    "\n",
    "            loss = policy_loss + value_loss + entropy_loss\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100 - Reward: 56.0\n",
      "Episode 200 - Reward: 85.0\n",
      "Episode 300 - Reward: 216.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "old_model = PPOActorCritic(state_dim, action_dim)\n",
    "new_model = PPOActorCritic(state_dim, action_dim)\n",
    "\n",
    "new_model.load_state_dict(old_model.state_dict()) # Synchronize models initially\n",
    "optimizer = optim.Adam([\n",
    "    {'params' : new_model.actor.parameters(), 'lr' : LEARNING_RATE},\n",
    "    {'params' : new_model.critic.parameters(), 'lr' : LEARNING_RATE}\n",
    "])\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "buffer.clear()\n",
    "\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    episode_reward = 0\n",
    "\n",
    "    buffer.clear()\n",
    "\n",
    "    for t in range(TIMESTEPS):\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, value, _ = old_model.get_action_and_value(state)\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "        # Store data\n",
    "        buffer.states.append(state.numpy())\n",
    "        buffer.actions.append(action.item())\n",
    "        buffer.log_probs.append(log_prob.item())\n",
    "        buffer.rewards.append(reward)\n",
    "        buffer.state_values.append(value.item())\n",
    "        buffer.dones.append(done)\n",
    "\n",
    "        state = torch.FloatTensor(next_state)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "            state = torch.FloatTensor(state)\n",
    "            break\n",
    "\n",
    "    # Train PPO\n",
    "    train_ppo(buffer, old_model, new_model, optimizer)\n",
    "    old_model.load_state_dict(new_model.state_dict()) # Update old_model to match new_model\n",
    "\n",
    "    if (episode % 100 == 0) and (episode != 0):\n",
    "        print(f\"Episode {episode} - Reward: {episode_reward}\")\n",
    "\n",
    "    # early stop\n",
    "    if (episode_reward > 1000):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1 \t\t Reward : 300.0\n",
      "Episode : 2 \t\t Reward : 300.0\n",
      "Episode : 3 \t\t Reward : 300.0\n",
      "Episode : 4 \t\t Reward : 300.0\n",
      "Episode : 5 \t\t Reward : 300.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "max_ep_len = 300\n",
    "\n",
    "total_test_episodes = 5\n",
    "test_running_reward = 0\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "for episode in range(1, total_test_episodes+1):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for t in range(max_ep_len):\n",
    "        action_probs = new_model.actor(torch.FloatTensor(state))\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        state, reward, done, truncated, _ = env.step(action.item())\n",
    "        ep_reward += reward\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        if done:\n",
    "            state, info = env.reset()\n",
    "    \n",
    "    test_running_reward += ep_reward\n",
    "    print(f\"Episode : {episode} \\t\\t Reward : {round(ep_reward, 2)}\")\n",
    "    ep_reward = 0\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
