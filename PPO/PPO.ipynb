{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameters\n",
    "\n",
    "LEARNING_RATE = 0.0003\n",
    "GAMMA = 0.99\n",
    "EPSILON_CLIP = 0.2\n",
    "ENTROPY_COEFF = 0.01\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "TIMESTEPS = 2048"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Action Critic Network\n",
    "\n",
    "class PPOActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(PPOActorCritic, self).__init__()\n",
    "    \n",
    "        # actor\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, action_dim),\n",
    "            nn.Softmax(dim = -1)\n",
    "        )\n",
    "\n",
    "        # critic\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(state_dim, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 64),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def get_action_and_value(self, state):\n",
    "\n",
    "        action_probs = self.actor(state)  # pi(a|s) => left|right => ex : [0.7, 0.3]\n",
    "        state_values = self.critic(state) # v(s), R + rV(s') - \"V(s)\"\n",
    "\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample() # left|right = [0.75, 0.25] => 0번 선택\n",
    "        action_logprobs = dist.log_prob(action) # log(0.75)\n",
    "        entropy = dist.entropy() # entropy([0.75, 0.25])\n",
    "\n",
    "        return action, action_logprobs, state_values, entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RolloutBuffer():\n",
    "    def __init__(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.dones = []\n",
    "\n",
    "    def clear(self):\n",
    "        self.actions = []\n",
    "        self.states = []\n",
    "        self.log_probs = []\n",
    "        self.rewards = []\n",
    "        self.state_values = []\n",
    "        self.dones = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(buffer, old_model, new_model, optimizer):\n",
    "    # calculate last state_value for : r + gamma * V(t)\n",
    "    state = buffer.states[-1]\n",
    "    done = buffer.dones[-1]\n",
    "    with torch.no_grad():\n",
    "        discounted_rewards = 0 if done else old_model.get_action_and_value(torch.FloatTensor(state))\n",
    "\n",
    "    returns = []\n",
    "    for reward in reversed(buffer.rewards):\n",
    "        discounted_rewards = reward + GAMMA * discounted_rewards\n",
    "        returns.insert(0, discounted_rewards)\n",
    "\n",
    "    advantages = torch.FloatTensor(returns) - torch.FloatTensor(buffer.state_values)\n",
    "\n",
    "    for _ in range(EPOCHS):\n",
    "        for idx in range(0, len(buffer.states), BATCH_SIZE):\n",
    "            batch_states = torch.FloatTensor(buffer.states[idx : idx + BATCH_SIZE])\n",
    "            batch_actions = torch.LongTensor(buffer.actions[idx : idx + BATCH_SIZE])\n",
    "\n",
    "            batch_returns = torch.FloatTensor(returns[idx : idx + BATCH_SIZE])\n",
    "            batch_advantages = torch.FloatTensor(advantages[idx : idx + BATCH_SIZE])\n",
    "\n",
    "            # new_model에서 새로운 정책 계산\n",
    "            new_policy_logits = new_model.actor(batch_states)\n",
    "            values = new_model.critic(batch_states)\n",
    "            new_policy_dist = Categorical(logits = new_policy_logits)\n",
    "            new_log_probs = new_policy_dist.log_prob(batch_actions)\n",
    "            entropy = new_policy_dist.entropy()\n",
    "\n",
    "            # old_model에서 이전 정책 계산\n",
    "            with torch.no_grad():\n",
    "                old_policy_logits = old_model.actor(batch_states)\n",
    "                old_policy_dist = Categorical(logits = old_policy_logits)\n",
    "                old_log_probs = old_policy_dist.log_prob(batch_actions)\n",
    "\n",
    "            # Compute ratio\n",
    "            ratios = torch.exp(new_log_probs - old_log_probs)\n",
    "\n",
    "            # PPO Loss\n",
    "            surrogate1 = ratios * batch_advantages\n",
    "            surrogate2 = torch.clamp(ratios, 1 - EPSILON_CLIP, 1 + EPSILON_CLIP) * batch_advantages\n",
    "            policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "\n",
    "            value_loss = nn.MSELoss()(values.squeeze(), batch_returns)\n",
    "\n",
    "            entropy_loss = - ENTROPY_COEFF * entropy.mean()\n",
    "\n",
    "            loss = policy_loss + value_loss + entropy_loss\n",
    "\n",
    "            # Optimize\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 - Reward: 22.0\n",
      "Episode 1 - Reward: 34.0\n",
      "Episode 2 - Reward: 24.0\n",
      "Episode 3 - Reward: 36.0\n",
      "Episode 4 - Reward: 19.0\n",
      "Episode 5 - Reward: 48.0\n",
      "Episode 6 - Reward: 55.0\n",
      "Episode 7 - Reward: 25.0\n",
      "Episode 8 - Reward: 11.0\n",
      "Episode 9 - Reward: 86.0\n",
      "Episode 10 - Reward: 29.0\n",
      "Episode 11 - Reward: 24.0\n",
      "Episode 12 - Reward: 45.0\n",
      "Episode 13 - Reward: 28.0\n",
      "Episode 14 - Reward: 55.0\n",
      "Episode 15 - Reward: 107.0\n",
      "Episode 16 - Reward: 44.0\n",
      "Episode 17 - Reward: 84.0\n",
      "Episode 18 - Reward: 55.0\n",
      "Episode 19 - Reward: 36.0\n",
      "Episode 20 - Reward: 48.0\n",
      "Episode 21 - Reward: 33.0\n",
      "Episode 22 - Reward: 23.0\n",
      "Episode 23 - Reward: 47.0\n",
      "Episode 24 - Reward: 31.0\n",
      "Episode 25 - Reward: 66.0\n",
      "Episode 26 - Reward: 66.0\n",
      "Episode 27 - Reward: 52.0\n",
      "Episode 28 - Reward: 57.0\n",
      "Episode 29 - Reward: 56.0\n",
      "Episode 30 - Reward: 59.0\n",
      "Episode 31 - Reward: 35.0\n",
      "Episode 32 - Reward: 58.0\n",
      "Episode 33 - Reward: 36.0\n",
      "Episode 34 - Reward: 33.0\n",
      "Episode 35 - Reward: 83.0\n",
      "Episode 36 - Reward: 39.0\n",
      "Episode 37 - Reward: 46.0\n",
      "Episode 38 - Reward: 37.0\n",
      "Episode 39 - Reward: 80.0\n",
      "Episode 40 - Reward: 38.0\n",
      "Episode 41 - Reward: 81.0\n",
      "Episode 42 - Reward: 63.0\n",
      "Episode 43 - Reward: 76.0\n",
      "Episode 44 - Reward: 51.0\n",
      "Episode 45 - Reward: 59.0\n",
      "Episode 46 - Reward: 32.0\n",
      "Episode 47 - Reward: 96.0\n",
      "Episode 48 - Reward: 73.0\n",
      "Episode 49 - Reward: 128.0\n",
      "Episode 50 - Reward: 109.0\n",
      "Episode 51 - Reward: 80.0\n",
      "Episode 52 - Reward: 112.0\n",
      "Episode 53 - Reward: 54.0\n",
      "Episode 54 - Reward: 130.0\n",
      "Episode 55 - Reward: 40.0\n",
      "Episode 56 - Reward: 64.0\n",
      "Episode 57 - Reward: 45.0\n",
      "Episode 58 - Reward: 78.0\n",
      "Episode 59 - Reward: 51.0\n",
      "Episode 60 - Reward: 51.0\n",
      "Episode 61 - Reward: 34.0\n",
      "Episode 62 - Reward: 39.0\n",
      "Episode 63 - Reward: 36.0\n",
      "Episode 64 - Reward: 63.0\n",
      "Episode 65 - Reward: 33.0\n",
      "Episode 66 - Reward: 34.0\n",
      "Episode 67 - Reward: 32.0\n",
      "Episode 68 - Reward: 64.0\n",
      "Episode 69 - Reward: 42.0\n",
      "Episode 70 - Reward: 36.0\n",
      "Episode 71 - Reward: 51.0\n",
      "Episode 72 - Reward: 41.0\n",
      "Episode 73 - Reward: 37.0\n",
      "Episode 74 - Reward: 44.0\n",
      "Episode 75 - Reward: 88.0\n",
      "Episode 76 - Reward: 99.0\n",
      "Episode 77 - Reward: 33.0\n",
      "Episode 78 - Reward: 26.0\n",
      "Episode 79 - Reward: 56.0\n",
      "Episode 80 - Reward: 43.0\n",
      "Episode 81 - Reward: 37.0\n",
      "Episode 82 - Reward: 42.0\n",
      "Episode 83 - Reward: 31.0\n",
      "Episode 84 - Reward: 41.0\n",
      "Episode 85 - Reward: 32.0\n",
      "Episode 86 - Reward: 36.0\n",
      "Episode 87 - Reward: 57.0\n",
      "Episode 88 - Reward: 41.0\n",
      "Episode 89 - Reward: 49.0\n",
      "Episode 90 - Reward: 64.0\n",
      "Episode 91 - Reward: 51.0\n",
      "Episode 92 - Reward: 48.0\n",
      "Episode 93 - Reward: 33.0\n",
      "Episode 94 - Reward: 39.0\n",
      "Episode 95 - Reward: 60.0\n",
      "Episode 96 - Reward: 114.0\n",
      "Episode 97 - Reward: 31.0\n",
      "Episode 98 - Reward: 71.0\n",
      "Episode 99 - Reward: 38.0\n",
      "Episode 100 - Reward: 92.0\n",
      "Episode 101 - Reward: 50.0\n",
      "Episode 102 - Reward: 93.0\n",
      "Episode 103 - Reward: 119.0\n",
      "Episode 104 - Reward: 49.0\n",
      "Episode 105 - Reward: 50.0\n",
      "Episode 106 - Reward: 53.0\n",
      "Episode 107 - Reward: 54.0\n",
      "Episode 108 - Reward: 49.0\n",
      "Episode 109 - Reward: 57.0\n",
      "Episode 110 - Reward: 71.0\n",
      "Episode 111 - Reward: 48.0\n",
      "Episode 112 - Reward: 75.0\n",
      "Episode 113 - Reward: 41.0\n",
      "Episode 114 - Reward: 31.0\n",
      "Episode 115 - Reward: 52.0\n",
      "Episode 116 - Reward: 45.0\n",
      "Episode 117 - Reward: 99.0\n",
      "Episode 118 - Reward: 36.0\n",
      "Episode 119 - Reward: 32.0\n",
      "Episode 120 - Reward: 29.0\n",
      "Episode 121 - Reward: 41.0\n",
      "Episode 122 - Reward: 35.0\n",
      "Episode 123 - Reward: 31.0\n",
      "Episode 124 - Reward: 28.0\n",
      "Episode 125 - Reward: 54.0\n",
      "Episode 126 - Reward: 64.0\n",
      "Episode 127 - Reward: 87.0\n",
      "Episode 128 - Reward: 43.0\n",
      "Episode 129 - Reward: 59.0\n",
      "Episode 130 - Reward: 89.0\n",
      "Episode 131 - Reward: 47.0\n",
      "Episode 132 - Reward: 117.0\n",
      "Episode 133 - Reward: 44.0\n",
      "Episode 134 - Reward: 50.0\n",
      "Episode 135 - Reward: 142.0\n",
      "Episode 136 - Reward: 50.0\n",
      "Episode 137 - Reward: 31.0\n",
      "Episode 138 - Reward: 41.0\n",
      "Episode 139 - Reward: 61.0\n",
      "Episode 140 - Reward: 53.0\n",
      "Episode 141 - Reward: 35.0\n",
      "Episode 142 - Reward: 35.0\n",
      "Episode 143 - Reward: 39.0\n",
      "Episode 144 - Reward: 41.0\n",
      "Episode 145 - Reward: 41.0\n",
      "Episode 146 - Reward: 39.0\n",
      "Episode 147 - Reward: 53.0\n",
      "Episode 148 - Reward: 101.0\n",
      "Episode 149 - Reward: 67.0\n",
      "Episode 150 - Reward: 92.0\n",
      "Episode 151 - Reward: 78.0\n",
      "Episode 152 - Reward: 40.0\n",
      "Episode 153 - Reward: 49.0\n",
      "Episode 154 - Reward: 44.0\n",
      "Episode 155 - Reward: 36.0\n",
      "Episode 156 - Reward: 157.0\n",
      "Episode 157 - Reward: 40.0\n",
      "Episode 158 - Reward: 41.0\n",
      "Episode 159 - Reward: 83.0\n",
      "Episode 160 - Reward: 39.0\n",
      "Episode 161 - Reward: 70.0\n",
      "Episode 162 - Reward: 66.0\n",
      "Episode 163 - Reward: 49.0\n",
      "Episode 164 - Reward: 36.0\n",
      "Episode 165 - Reward: 73.0\n",
      "Episode 166 - Reward: 64.0\n",
      "Episode 167 - Reward: 33.0\n",
      "Episode 168 - Reward: 49.0\n",
      "Episode 169 - Reward: 34.0\n",
      "Episode 170 - Reward: 47.0\n",
      "Episode 171 - Reward: 64.0\n",
      "Episode 172 - Reward: 40.0\n",
      "Episode 173 - Reward: 64.0\n",
      "Episode 174 - Reward: 29.0\n",
      "Episode 175 - Reward: 35.0\n",
      "Episode 176 - Reward: 78.0\n",
      "Episode 177 - Reward: 49.0\n",
      "Episode 178 - Reward: 81.0\n",
      "Episode 179 - Reward: 48.0\n",
      "Episode 180 - Reward: 85.0\n",
      "Episode 181 - Reward: 39.0\n",
      "Episode 182 - Reward: 42.0\n",
      "Episode 183 - Reward: 32.0\n",
      "Episode 184 - Reward: 51.0\n",
      "Episode 185 - Reward: 64.0\n",
      "Episode 186 - Reward: 34.0\n",
      "Episode 187 - Reward: 105.0\n",
      "Episode 188 - Reward: 71.0\n",
      "Episode 189 - Reward: 46.0\n",
      "Episode 190 - Reward: 45.0\n",
      "Episode 191 - Reward: 85.0\n",
      "Episode 192 - Reward: 46.0\n",
      "Episode 193 - Reward: 54.0\n",
      "Episode 194 - Reward: 45.0\n",
      "Episode 195 - Reward: 41.0\n",
      "Episode 196 - Reward: 106.0\n",
      "Episode 197 - Reward: 70.0\n",
      "Episode 198 - Reward: 93.0\n",
      "Episode 199 - Reward: 79.0\n",
      "Episode 200 - Reward: 37.0\n",
      "Episode 201 - Reward: 31.0\n",
      "Episode 202 - Reward: 41.0\n",
      "Episode 203 - Reward: 55.0\n",
      "Episode 204 - Reward: 53.0\n",
      "Episode 205 - Reward: 47.0\n",
      "Episode 206 - Reward: 66.0\n",
      "Episode 207 - Reward: 40.0\n",
      "Episode 208 - Reward: 93.0\n",
      "Episode 209 - Reward: 51.0\n",
      "Episode 210 - Reward: 33.0\n",
      "Episode 211 - Reward: 60.0\n",
      "Episode 212 - Reward: 32.0\n",
      "Episode 213 - Reward: 66.0\n",
      "Episode 214 - Reward: 47.0\n",
      "Episode 215 - Reward: 52.0\n",
      "Episode 216 - Reward: 38.0\n",
      "Episode 217 - Reward: 39.0\n",
      "Episode 218 - Reward: 40.0\n",
      "Episode 219 - Reward: 76.0\n",
      "Episode 220 - Reward: 60.0\n",
      "Episode 221 - Reward: 37.0\n",
      "Episode 222 - Reward: 56.0\n",
      "Episode 223 - Reward: 40.0\n",
      "Episode 224 - Reward: 49.0\n",
      "Episode 225 - Reward: 96.0\n",
      "Episode 226 - Reward: 48.0\n",
      "Episode 227 - Reward: 51.0\n",
      "Episode 228 - Reward: 102.0\n",
      "Episode 229 - Reward: 60.0\n",
      "Episode 230 - Reward: 55.0\n",
      "Episode 231 - Reward: 78.0\n",
      "Episode 232 - Reward: 60.0\n",
      "Episode 233 - Reward: 75.0\n",
      "Episode 234 - Reward: 140.0\n",
      "Episode 235 - Reward: 74.0\n",
      "Episode 236 - Reward: 58.0\n",
      "Episode 237 - Reward: 122.0\n",
      "Episode 238 - Reward: 145.0\n",
      "Episode 239 - Reward: 64.0\n",
      "Episode 240 - Reward: 121.0\n",
      "Episode 241 - Reward: 150.0\n",
      "Episode 242 - Reward: 77.0\n",
      "Episode 243 - Reward: 115.0\n",
      "Episode 244 - Reward: 60.0\n",
      "Episode 245 - Reward: 235.0\n",
      "Episode 246 - Reward: 146.0\n",
      "Episode 247 - Reward: 100.0\n",
      "Episode 248 - Reward: 54.0\n",
      "Episode 249 - Reward: 215.0\n",
      "Episode 250 - Reward: 182.0\n",
      "Episode 251 - Reward: 91.0\n",
      "Episode 252 - Reward: 55.0\n",
      "Episode 253 - Reward: 56.0\n",
      "Episode 254 - Reward: 91.0\n",
      "Episode 255 - Reward: 144.0\n",
      "Episode 256 - Reward: 49.0\n",
      "Episode 257 - Reward: 67.0\n",
      "Episode 258 - Reward: 41.0\n",
      "Episode 259 - Reward: 114.0\n",
      "Episode 260 - Reward: 38.0\n",
      "Episode 261 - Reward: 112.0\n",
      "Episode 262 - Reward: 49.0\n",
      "Episode 263 - Reward: 49.0\n",
      "Episode 264 - Reward: 44.0\n",
      "Episode 265 - Reward: 91.0\n",
      "Episode 266 - Reward: 43.0\n",
      "Episode 267 - Reward: 52.0\n",
      "Episode 268 - Reward: 65.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Noh\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:535: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 269 - Reward: 55.0\n",
      "Episode 270 - Reward: 75.0\n",
      "Episode 271 - Reward: 51.0\n",
      "Episode 272 - Reward: 46.0\n",
      "Episode 273 - Reward: 54.0\n",
      "Episode 274 - Reward: 98.0\n",
      "Episode 275 - Reward: 52.0\n",
      "Episode 276 - Reward: 56.0\n",
      "Episode 277 - Reward: 58.0\n",
      "Episode 278 - Reward: 56.0\n",
      "Episode 279 - Reward: 51.0\n",
      "Episode 280 - Reward: 38.0\n",
      "Episode 281 - Reward: 58.0\n",
      "Episode 282 - Reward: 37.0\n",
      "Episode 283 - Reward: 40.0\n",
      "Episode 284 - Reward: 46.0\n",
      "Episode 285 - Reward: 99.0\n",
      "Episode 286 - Reward: 62.0\n",
      "Episode 287 - Reward: 42.0\n",
      "Episode 288 - Reward: 66.0\n",
      "Episode 289 - Reward: 56.0\n",
      "Episode 290 - Reward: 38.0\n",
      "Episode 291 - Reward: 54.0\n",
      "Episode 292 - Reward: 80.0\n",
      "Episode 293 - Reward: 100.0\n",
      "Episode 294 - Reward: 93.0\n",
      "Episode 295 - Reward: 134.0\n",
      "Episode 296 - Reward: 92.0\n",
      "Episode 297 - Reward: 109.0\n",
      "Episode 298 - Reward: 71.0\n",
      "Episode 299 - Reward: 83.0\n",
      "Episode 300 - Reward: 81.0\n",
      "Episode 301 - Reward: 79.0\n",
      "Episode 302 - Reward: 46.0\n",
      "Episode 303 - Reward: 60.0\n",
      "Episode 304 - Reward: 46.0\n",
      "Episode 305 - Reward: 208.0\n",
      "Episode 306 - Reward: 68.0\n",
      "Episode 307 - Reward: 64.0\n",
      "Episode 308 - Reward: 54.0\n",
      "Episode 309 - Reward: 47.0\n",
      "Episode 310 - Reward: 97.0\n",
      "Episode 311 - Reward: 71.0\n",
      "Episode 312 - Reward: 57.0\n",
      "Episode 313 - Reward: 50.0\n",
      "Episode 314 - Reward: 59.0\n",
      "Episode 315 - Reward: 94.0\n",
      "Episode 316 - Reward: 62.0\n",
      "Episode 317 - Reward: 141.0\n",
      "Episode 318 - Reward: 60.0\n",
      "Episode 319 - Reward: 84.0\n",
      "Episode 320 - Reward: 42.0\n",
      "Episode 321 - Reward: 86.0\n",
      "Episode 322 - Reward: 59.0\n",
      "Episode 323 - Reward: 262.0\n",
      "Episode 324 - Reward: 249.0\n",
      "Episode 325 - Reward: 54.0\n",
      "Episode 326 - Reward: 50.0\n",
      "Episode 327 - Reward: 53.0\n",
      "Episode 328 - Reward: 114.0\n",
      "Episode 329 - Reward: 82.0\n",
      "Episode 330 - Reward: 46.0\n",
      "Episode 331 - Reward: 88.0\n",
      "Episode 332 - Reward: 42.0\n",
      "Episode 333 - Reward: 38.0\n",
      "Episode 334 - Reward: 60.0\n",
      "Episode 335 - Reward: 39.0\n",
      "Episode 336 - Reward: 50.0\n",
      "Episode 337 - Reward: 39.0\n",
      "Episode 338 - Reward: 39.0\n",
      "Episode 339 - Reward: 63.0\n",
      "Episode 340 - Reward: 40.0\n",
      "Episode 341 - Reward: 113.0\n",
      "Episode 342 - Reward: 52.0\n",
      "Episode 343 - Reward: 62.0\n",
      "Episode 344 - Reward: 68.0\n",
      "Episode 345 - Reward: 61.0\n",
      "Episode 346 - Reward: 58.0\n",
      "Episode 347 - Reward: 60.0\n",
      "Episode 348 - Reward: 50.0\n",
      "Episode 349 - Reward: 175.0\n",
      "Episode 350 - Reward: 135.0\n",
      "Episode 351 - Reward: 160.0\n",
      "Episode 352 - Reward: 106.0\n",
      "Episode 353 - Reward: 58.0\n",
      "Episode 354 - Reward: 57.0\n",
      "Episode 355 - Reward: 42.0\n",
      "Episode 356 - Reward: 109.0\n",
      "Episode 357 - Reward: 44.0\n",
      "Episode 358 - Reward: 60.0\n",
      "Episode 359 - Reward: 41.0\n",
      "Episode 360 - Reward: 42.0\n",
      "Episode 361 - Reward: 46.0\n",
      "Episode 362 - Reward: 121.0\n",
      "Episode 363 - Reward: 37.0\n",
      "Episode 364 - Reward: 43.0\n",
      "Episode 365 - Reward: 39.0\n",
      "Episode 366 - Reward: 60.0\n",
      "Episode 367 - Reward: 59.0\n",
      "Episode 368 - Reward: 191.0\n",
      "Episode 369 - Reward: 51.0\n",
      "Episode 370 - Reward: 68.0\n",
      "Episode 371 - Reward: 45.0\n",
      "Episode 372 - Reward: 63.0\n",
      "Episode 373 - Reward: 43.0\n",
      "Episode 374 - Reward: 50.0\n",
      "Episode 375 - Reward: 181.0\n",
      "Episode 376 - Reward: 57.0\n",
      "Episode 377 - Reward: 42.0\n",
      "Episode 378 - Reward: 43.0\n",
      "Episode 379 - Reward: 44.0\n",
      "Episode 380 - Reward: 170.0\n",
      "Episode 381 - Reward: 92.0\n",
      "Episode 382 - Reward: 45.0\n",
      "Episode 383 - Reward: 54.0\n",
      "Episode 384 - Reward: 51.0\n",
      "Episode 385 - Reward: 44.0\n",
      "Episode 386 - Reward: 41.0\n",
      "Episode 387 - Reward: 36.0\n",
      "Episode 388 - Reward: 34.0\n",
      "Episode 389 - Reward: 77.0\n",
      "Episode 390 - Reward: 43.0\n",
      "Episode 391 - Reward: 40.0\n",
      "Episode 392 - Reward: 40.0\n",
      "Episode 393 - Reward: 37.0\n",
      "Episode 394 - Reward: 29.0\n",
      "Episode 395 - Reward: 85.0\n",
      "Episode 396 - Reward: 34.0\n",
      "Episode 397 - Reward: 45.0\n",
      "Episode 398 - Reward: 45.0\n",
      "Episode 399 - Reward: 35.0\n",
      "Episode 400 - Reward: 231.0\n",
      "Episode 401 - Reward: 50.0\n",
      "Episode 402 - Reward: 42.0\n",
      "Episode 403 - Reward: 58.0\n",
      "Episode 404 - Reward: 63.0\n",
      "Episode 405 - Reward: 47.0\n",
      "Episode 406 - Reward: 122.0\n",
      "Episode 407 - Reward: 188.0\n",
      "Episode 408 - Reward: 47.0\n",
      "Episode 409 - Reward: 56.0\n",
      "Episode 410 - Reward: 58.0\n",
      "Episode 411 - Reward: 129.0\n",
      "Episode 412 - Reward: 113.0\n",
      "Episode 413 - Reward: 114.0\n",
      "Episode 414 - Reward: 46.0\n",
      "Episode 415 - Reward: 53.0\n",
      "Episode 416 - Reward: 53.0\n",
      "Episode 417 - Reward: 45.0\n",
      "Episode 418 - Reward: 91.0\n",
      "Episode 419 - Reward: 46.0\n",
      "Episode 420 - Reward: 70.0\n",
      "Episode 421 - Reward: 57.0\n",
      "Episode 422 - Reward: 228.0\n",
      "Episode 423 - Reward: 63.0\n",
      "Episode 424 - Reward: 71.0\n",
      "Episode 425 - Reward: 49.0\n",
      "Episode 426 - Reward: 79.0\n",
      "Episode 427 - Reward: 91.0\n",
      "Episode 428 - Reward: 80.0\n",
      "Episode 429 - Reward: 74.0\n",
      "Episode 430 - Reward: 99.0\n",
      "Episode 431 - Reward: 107.0\n",
      "Episode 432 - Reward: 72.0\n",
      "Episode 433 - Reward: 74.0\n",
      "Episode 434 - Reward: 106.0\n",
      "Episode 435 - Reward: 602.0\n",
      "Episode 436 - Reward: 114.0\n",
      "Episode 437 - Reward: 136.0\n",
      "Episode 438 - Reward: 189.0\n",
      "Episode 439 - Reward: 69.0\n",
      "Episode 440 - Reward: 102.0\n",
      "Episode 441 - Reward: 123.0\n",
      "Episode 442 - Reward: 106.0\n",
      "Episode 443 - Reward: 160.0\n",
      "Episode 444 - Reward: 66.0\n",
      "Episode 445 - Reward: 96.0\n",
      "Episode 446 - Reward: 118.0\n",
      "Episode 447 - Reward: 92.0\n",
      "Episode 448 - Reward: 229.0\n",
      "Episode 449 - Reward: 100.0\n",
      "Episode 450 - Reward: 233.0\n",
      "Episode 451 - Reward: 100.0\n",
      "Episode 452 - Reward: 81.0\n",
      "Episode 453 - Reward: 136.0\n",
      "Episode 454 - Reward: 64.0\n",
      "Episode 455 - Reward: 94.0\n",
      "Episode 456 - Reward: 509.0\n",
      "Episode 457 - Reward: 230.0\n",
      "Episode 458 - Reward: 87.0\n",
      "Episode 459 - Reward: 83.0\n",
      "Episode 460 - Reward: 159.0\n",
      "Episode 461 - Reward: 101.0\n",
      "Episode 462 - Reward: 501.0\n",
      "Episode 463 - Reward: 746.0\n",
      "Episode 464 - Reward: 92.0\n",
      "Episode 465 - Reward: 84.0\n",
      "Episode 466 - Reward: 242.0\n",
      "Episode 467 - Reward: 127.0\n",
      "Episode 468 - Reward: 165.0\n",
      "Episode 469 - Reward: 350.0\n",
      "Episode 470 - Reward: 126.0\n",
      "Episode 471 - Reward: 153.0\n",
      "Episode 472 - Reward: 606.0\n",
      "Episode 473 - Reward: 200.0\n",
      "Episode 474 - Reward: 148.0\n",
      "Episode 475 - Reward: 114.0\n",
      "Episode 476 - Reward: 343.0\n",
      "Episode 477 - Reward: 97.0\n",
      "Episode 478 - Reward: 118.0\n",
      "Episode 479 - Reward: 426.0\n",
      "Episode 480 - Reward: 212.0\n",
      "Episode 481 - Reward: 142.0\n",
      "Episode 482 - Reward: 431.0\n",
      "Episode 483 - Reward: 142.0\n",
      "Episode 484 - Reward: 266.0\n",
      "Episode 485 - Reward: 240.0\n",
      "Episode 486 - Reward: 138.0\n",
      "Episode 487 - Reward: 183.0\n",
      "Episode 488 - Reward: 167.0\n",
      "Episode 489 - Reward: 369.0\n",
      "Episode 490 - Reward: 371.0\n",
      "Episode 491 - Reward: 389.0\n",
      "Episode 492 - Reward: 325.0\n",
      "Episode 493 - Reward: 409.0\n",
      "Episode 494 - Reward: 549.0\n",
      "Episode 495 - Reward: 342.0\n",
      "Episode 496 - Reward: 575.0\n",
      "Episode 497 - Reward: 311.0\n",
      "Episode 498 - Reward: 305.0\n",
      "Episode 499 - Reward: 477.0\n",
      "Episode 500 - Reward: 275.0\n",
      "Episode 501 - Reward: 279.0\n",
      "Episode 502 - Reward: 279.0\n",
      "Episode 503 - Reward: 240.0\n",
      "Episode 504 - Reward: 282.0\n",
      "Episode 505 - Reward: 206.0\n",
      "Episode 506 - Reward: 279.0\n",
      "Episode 507 - Reward: 400.0\n",
      "Episode 508 - Reward: 427.0\n",
      "Episode 509 - Reward: 763.0\n",
      "Episode 510 - Reward: 383.0\n",
      "Episode 511 - Reward: 413.0\n",
      "Episode 512 - Reward: 379.0\n",
      "Episode 513 - Reward: 290.0\n",
      "Episode 514 - Reward: 385.0\n",
      "Episode 515 - Reward: 372.0\n",
      "Episode 516 - Reward: 545.0\n",
      "Episode 517 - Reward: 411.0\n",
      "Episode 518 - Reward: 445.0\n",
      "Episode 519 - Reward: 341.0\n",
      "Episode 520 - Reward: 667.0\n",
      "Episode 521 - Reward: 457.0\n",
      "Episode 522 - Reward: 271.0\n",
      "Episode 523 - Reward: 204.0\n",
      "Episode 524 - Reward: 335.0\n",
      "Episode 525 - Reward: 237.0\n",
      "Episode 526 - Reward: 238.0\n",
      "Episode 527 - Reward: 369.0\n",
      "Episode 528 - Reward: 236.0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 46\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Train PPO\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m train_ppo(buffer, old_model, new_model, optimizer)\n\u001b[0;32m     47\u001b[0m old_model\u001b[38;5;241m.\u001b[39mload_state_dict(new_model\u001b[38;5;241m.\u001b[39mstate_dict()) \u001b[38;5;66;03m# Update old_model to match new_model\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_reward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[5], line 10\u001b[0m, in \u001b[0;36mtrain_ppo\u001b[1;34m(buffer, old_model, new_model, optimizer)\u001b[0m\n\u001b[0;32m      8\u001b[0m returns \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m reward \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(buffer\u001b[38;5;241m.\u001b[39mrewards):\n\u001b[1;32m---> 10\u001b[0m     discounted_rewards \u001b[38;5;241m=\u001b[39m reward \u001b[38;5;241m+\u001b[39m GAMMA \u001b[38;5;241m*\u001b[39m discounted_rewards\n\u001b[0;32m     11\u001b[0m     returns\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, discounted_rewards)\n\u001b[0;32m     13\u001b[0m advantages \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(returns) \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(buffer\u001b[38;5;241m.\u001b[39mstate_values)\n",
      "\u001b[1;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "old_model = PPOActorCritic(state_dim, action_dim)\n",
    "new_model = PPOActorCritic(state_dim, action_dim)\n",
    "\n",
    "new_model.load_state_dict(old_model.state_dict()) # Synchronize models initially\n",
    "optimizer = optim.Adam([\n",
    "    {'params' : new_model.actor.parameters(), 'lr' : LEARNING_RATE},\n",
    "    {'params' : new_model.critic.parameters(), 'lr' : LEARNING_RATE}\n",
    "])\n",
    "\n",
    "buffer = RolloutBuffer()\n",
    "buffer.clear()\n",
    "\n",
    "for episode in range(1000):\n",
    "    state, _ = env.reset()\n",
    "    state = torch.FloatTensor(state)\n",
    "    episode_reward = 0\n",
    "\n",
    "    buffer.clear()\n",
    "\n",
    "    for t in range(TIMESTEPS):\n",
    "        with torch.no_grad():\n",
    "            action, log_prob, value, _ = old_model.get_action_and_value(state)\n",
    "            next_state, reward, done, _, _ = env.step(action.item())\n",
    "\n",
    "        # Store data\n",
    "        buffer.states.append(state.numpy())\n",
    "        buffer.actions.append(action.item())\n",
    "        buffer.log_probs.append(log_prob.item())\n",
    "        buffer.rewards.append(reward)\n",
    "        buffer.state_values.append(value.item())\n",
    "        buffer.dones.append(done)\n",
    "\n",
    "        state = torch.FloatTensor(next_state)\n",
    "        episode_reward += reward\n",
    "\n",
    "        if done:\n",
    "            state, _ = env.reset()\n",
    "            state = torch.FloatTensor(state)\n",
    "            break\n",
    "\n",
    "    # Train PPO\n",
    "    train_ppo(buffer, old_model, new_model, optimizer)\n",
    "    old_model.load_state_dict(new_model.state_dict()) # Update old_model to match new_model\n",
    "\n",
    "    print(f\"Episode {episode} - Reward: {episode_reward}\")\n",
    "\n",
    "    if (episode_reward > 1000):\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(500):\n",
    "    s = torch.from_numpy(state).float().unsqueeze(0)\n",
    "    action, lp, m, entropy = old_model.get_action_and_value(s)\n",
    "    state, reward, done, truncated, _ = env.step(action.item())\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    if done:\n",
    "        state, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode : 1 \t\t Reward : 300.0\n",
      "Episode : 2 \t\t Reward : 300.0\n",
      "Episode : 3 \t\t Reward : 300.0\n",
      "Episode : 4 \t\t Reward : 300.0\n",
      "Episode : 5 \t\t Reward : 300.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "max_ep_len = 300\n",
    "\n",
    "total_test_episodes = 5\n",
    "test_running_reward = 0\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "\n",
    "for episode in range(1, total_test_episodes+1):\n",
    "    state, _ = env.reset()\n",
    "    ep_reward = 0\n",
    "\n",
    "    for t in range(max_ep_len):\n",
    "        action_probs = new_model.actor(torch.FloatTensor(state))\n",
    "        dist = Categorical(action_probs)\n",
    "        action = dist.sample()\n",
    "\n",
    "        state, reward, done, truncated, _ = env.step(action.item())\n",
    "        ep_reward += reward\n",
    "\n",
    "        env.render()\n",
    "        time.sleep(0.01)\n",
    "\n",
    "        if done:\n",
    "            state, info = env.reset()\n",
    "    \n",
    "    test_running_reward += ep_reward\n",
    "    print(f\"Episode : {episode} \\t\\t Reward : {round(ep_reward, 2)}\")\n",
    "    ep_reward = 0\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
