{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(8, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "\n",
    "        return self.layer3(x)\n",
    "\n",
    "    def sample_action(self, state, eps):\n",
    "        out = self.forward(state)\n",
    "        coin = random.random()\n",
    "        if (coin < eps):\n",
    "            return random.randint(0, 3)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device : {device}\")\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "q_net = DQN().to(device)\n",
    "q_target_net = DQN().to(device)\n",
    "q_target_net.load_state_dict(q_net.state_dict())\n",
    "optimizer = optim.Adam(q_net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "\n",
    "import random\n",
    "import collections\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = collections.deque(maxlen = buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_list, a_list, r_list, s_next_list, done_list = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_next, done = transition\n",
    "            s_list.append(s)\n",
    "            a_list.append([a])\n",
    "            r_list.append([r])\n",
    "            s_next_list.append(s_next)\n",
    "            done_list.append([done])\n",
    "\n",
    "        return torch.tensor(s_list, dtype = torch.float, device = device), \\\n",
    "                torch.tensor(a_list, device = device), \\\n",
    "                torch.tensor(r_list, device = device), \\\n",
    "                torch.tensor(s_next_list, device = device), \\\n",
    "                torch.tensor(done_list, device = device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "memory = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "batch_size = 128\n",
    "gamma = 1.0\n",
    "\n",
    "def model_trainer(q_net, q_target_net, memory):\n",
    "    for i in range(10):\n",
    "        state, action, reward, s_next, done = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q_net(state)\n",
    "        q_a = q_out.gather(1, action)\n",
    "\n",
    "        max_q_a_next = q_target_net(s_next).max(1)[0].unsqueeze(1)\n",
    "        target = reward + gamma * max_q_a_next * done\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 20, score : -568.2045061402252, n_buffer : 1495, eps : 7.9%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noh\\AppData\\Local\\Temp\\ipykernel_16468\\3885697836.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  return torch.tensor(s_list, dtype = torch.float, device = device), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 40, score : -290.19477660820235, n_buffer : 2918, eps : 7.8%\n",
      "n_episode : 60, score : -162.43175479450514, n_buffer : 4383, eps : 7.7%\n",
      "n_episode : 80, score : -225.92103225452306, n_buffer : 6129, eps : 7.6%\n",
      "n_episode : 100, score : -226.61103217731366, n_buffer : 9713, eps : 7.5%\n",
      "n_episode : 120, score : -268.76925679857476, n_buffer : 10000, eps : 7.4%\n",
      "n_episode : 140, score : -365.6818950165064, n_buffer : 10000, eps : 7.3%\n",
      "n_episode : 160, score : -137.39174828285238, n_buffer : 10000, eps : 7.2%\n",
      "n_episode : 180, score : -86.76104174502817, n_buffer : 10000, eps : 7.1%\n",
      "n_episode : 200, score : -111.90979883684827, n_buffer : 10000, eps : 7.0%\n",
      "n_episode : 220, score : -89.57376791004563, n_buffer : 10000, eps : 6.9%\n",
      "n_episode : 240, score : -115.43991211166613, n_buffer : 10000, eps : 6.8%\n",
      "n_episode : 260, score : -64.51456311919992, n_buffer : 10000, eps : 6.7%\n",
      "n_episode : 280, score : -210.01558630689743, n_buffer : 10000, eps : 6.6%\n",
      "n_episode : 300, score : -90.95825541163775, n_buffer : 10000, eps : 6.5%\n",
      "n_episode : 320, score : -408.1925931675036, n_buffer : 10000, eps : 6.4%\n",
      "n_episode : 340, score : -477.9135748297419, n_buffer : 10000, eps : 6.3%\n",
      "n_episode : 360, score : -339.2909614728919, n_buffer : 10000, eps : 6.2%\n",
      "n_episode : 380, score : -561.3785511415374, n_buffer : 10000, eps : 6.1%\n",
      "n_episode : 400, score : -360.06804536962056, n_buffer : 10000, eps : 6.0%\n",
      "n_episode : 420, score : -482.26620837228836, n_buffer : 10000, eps : 5.9%\n",
      "n_episode : 440, score : -285.51584477950604, n_buffer : 10000, eps : 5.8%\n",
      "n_episode : 460, score : -290.1765871823826, n_buffer : 10000, eps : 5.7%\n",
      "n_episode : 480, score : -171.1010046451582, n_buffer : 10000, eps : 5.6%\n",
      "n_episode : 500, score : -161.85295378250652, n_buffer : 10000, eps : 5.5%\n",
      "n_episode : 520, score : -195.59979081223102, n_buffer : 10000, eps : 5.4%\n",
      "n_episode : 540, score : -51.90264968816432, n_buffer : 10000, eps : 5.3%\n",
      "n_episode : 560, score : -52.41447545911497, n_buffer : 10000, eps : 5.2%\n",
      "n_episode : 580, score : 31.332976718881003, n_buffer : 10000, eps : 5.1%\n",
      "n_episode : 600, score : -19.358914051946197, n_buffer : 10000, eps : 5.0%\n",
      "n_episode : 620, score : -30.182318266166313, n_buffer : 10000, eps : 4.9%\n",
      "n_episode : 640, score : -23.471940590989266, n_buffer : 10000, eps : 4.8%\n",
      "n_episode : 660, score : 2.7006755754861205, n_buffer : 10000, eps : 4.7%\n",
      "n_episode : 680, score : -31.790587310301902, n_buffer : 10000, eps : 4.6%\n",
      "n_episode : 700, score : 3.894491168652972, n_buffer : 10000, eps : 4.5%\n",
      "n_episode : 720, score : 18.459755586704468, n_buffer : 10000, eps : 4.4%\n",
      "n_episode : 740, score : -4.618559500352375, n_buffer : 10000, eps : 4.3%\n",
      "n_episode : 760, score : 4.51936194851358, n_buffer : 10000, eps : 4.2%\n",
      "n_episode : 780, score : -4.006223995231018, n_buffer : 10000, eps : 4.1%\n",
      "n_episode : 800, score : 37.05112685665967, n_buffer : 10000, eps : 4.0%\n",
      "n_episode : 820, score : 38.198230083292025, n_buffer : 10000, eps : 3.9%\n",
      "n_episode : 840, score : -11.115300082011306, n_buffer : 10000, eps : 3.8%\n",
      "n_episode : 860, score : 47.33867317260907, n_buffer : 10000, eps : 3.7%\n",
      "n_episode : 880, score : 56.72078118304275, n_buffer : 10000, eps : 3.6%\n",
      "n_episode : 900, score : 47.41491923482534, n_buffer : 10000, eps : 3.5%\n",
      "n_episode : 920, score : 54.578379670184475, n_buffer : 10000, eps : 3.4%\n",
      "n_episode : 940, score : 16.340245616018155, n_buffer : 10000, eps : 3.3%\n",
      "n_episode : 960, score : 21.049903316898742, n_buffer : 10000, eps : 3.2%\n",
      "n_episode : 980, score : 79.84712772268344, n_buffer : 10000, eps : 3.1%\n",
      "n_episode : 1000, score : 20.397962834086535, n_buffer : 10000, eps : 3.0%\n",
      "n_episode : 1020, score : 23.087436457163403, n_buffer : 10000, eps : 2.9%\n",
      "n_episode : 1040, score : 46.94016004546621, n_buffer : 10000, eps : 2.8%\n",
      "n_episode : 1060, score : 98.92971681201539, n_buffer : 10000, eps : 2.7%\n",
      "n_episode : 1080, score : 93.81638293789786, n_buffer : 10000, eps : 2.6%\n",
      "n_episode : 1100, score : 80.30245506392762, n_buffer : 10000, eps : 2.5%\n",
      "n_episode : 1120, score : 70.1251688338795, n_buffer : 10000, eps : 2.4%\n",
      "n_episode : 1140, score : 72.36605563611317, n_buffer : 10000, eps : 2.3%\n",
      "n_episode : 1160, score : 64.0392884217292, n_buffer : 10000, eps : 2.2%\n",
      "n_episode : 1180, score : 20.731610718517693, n_buffer : 10000, eps : 2.1%\n",
      "n_episode : 1200, score : 99.82938304986128, n_buffer : 10000, eps : 2.0%\n",
      "n_episode : 1220, score : 13.390904856830977, n_buffer : 10000, eps : 1.9%\n",
      "n_episode : 1240, score : 42.87730833590319, n_buffer : 10000, eps : 1.8%\n",
      "n_episode : 1260, score : 53.15967652195557, n_buffer : 10000, eps : 1.7%\n",
      "n_episode : 1280, score : 32.96411126859362, n_buffer : 10000, eps : 1.6%\n",
      "n_episode : 1300, score : 23.38548494900669, n_buffer : 10000, eps : 1.5%\n",
      "n_episode : 1320, score : 61.31434062672979, n_buffer : 10000, eps : 1.4%\n",
      "n_episode : 1340, score : 11.685077556231708, n_buffer : 10000, eps : 1.3%\n",
      "n_episode : 1360, score : 34.00463264488424, n_buffer : 10000, eps : 1.2%\n",
      "n_episode : 1380, score : 99.28433127478847, n_buffer : 10000, eps : 1.1%\n",
      "n_episode : 1400, score : 27.136028887389095, n_buffer : 10000, eps : 1.0%\n",
      "n_episode : 1420, score : 4.917090933019648, n_buffer : 10000, eps : 1.0%\n",
      "n_episode : 1440, score : -45.89247126314222, n_buffer : 10000, eps : 1.0%\n",
      "n_episode : 1460, score : -26.476779482039813, n_buffer : 10000, eps : 1.0%\n",
      "n_episode : 1480, score : 21.98570135466118, n_buffer : 10000, eps : 1.0%\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5)\n",
    "\n",
    "step_count = 1\n",
    "score = 0\n",
    "print_interval = 20\n",
    "\n",
    "for n_epi in range(1500):\n",
    "    epsilon = max(0.01, 0.08 - 0.01 * (n_epi / 200))\n",
    "    s, info = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = q_net.sample_action(torch.from_numpy(s).float().to(device), epsilon)\n",
    "\n",
    "        s_next, reward, done, truncated, info = env.step(a)\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "\n",
    "        memory.put((s, a, reward, s_next, done_mask))\n",
    "        s = s_next\n",
    "        score += reward\n",
    "\n",
    "        if (score / step_count > 300):\n",
    "            break\n",
    "\n",
    "        if (done):\n",
    "            break\n",
    "\n",
    "    step_count += 1\n",
    "    if (memory.size() > 2000):\n",
    "        model_trainer(q_net, q_target_net, memory)\n",
    "\n",
    "    if (n_epi % print_interval == 0) and (n_epi > 0):\n",
    "        q_target_net.load_state_dict(q_net.state_dict())\n",
    "        print(\"n_episode : {}, score : {}, n_buffer : {}, eps : {:.1f}%\".format(n_epi, score / print_interval, memory.size(), epsilon * 100))\n",
    "\n",
    "        score = 0\n",
    "        step_count = 1\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "\n",
    "import time\n",
    "env = gym.make(\"LunarLander-v3\", render_mode = \"human\", continuous=False, gravity=-10.0, enable_wind=False, wind_power=15.0, turbulence_power=1.5)\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype = torch.float32, device = device).unsqueeze(0)\n",
    "\n",
    "for i in range(1000):\n",
    "    action = q_net.sample_action(state, epsilon)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    state = next_state\n",
    "    state = torch.tensor(state, dtype = torch.float32, device = device).unsqueeze(0)\n",
    "    if (terminated):\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype = torch.float32, device = device).unsqueeze(0)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
