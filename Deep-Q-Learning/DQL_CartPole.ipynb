{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\noh\\anaconda3\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\noh\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\noh\\anaconda3\\lib\\site-packages (from gymnasium) (4.9.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
      "   ---------------------------------------- 0.0/958.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/958.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/958.1 kB ? eta -:--:--\n",
      "   - ------------------------------------- 41.0/958.1 kB 495.5 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 133.1/958.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 450.6/958.1 kB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 839.7/958.1 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  952.3/958.1 kB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 958.1/958.1 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. make\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. reset\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'action' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 3. step\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'action' is not defined"
     ]
    }
   ],
   "source": [
    "# 3. step\n",
    "observation, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. render(model)\n",
    "\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    action = 0\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "    state = next_state\n",
    "\n",
    "    if (terminated):\n",
    "        state, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "\n",
    "        return self.layer3(x)\n",
    "\n",
    "    def sample_action(self, state, eps):\n",
    "        out = self.forward(state)\n",
    "        coin = random.random()\n",
    "        if (coin < eps):\n",
    "            return random.randint(0, 1)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device : {device}\")\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "q_net = DQN().to(device)\n",
    "q_target_net = DQN().to(device)\n",
    "q_target_net.load_state_dict(q_net.state_dict())\n",
    "optimizer = optim.Adam(q_net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "\n",
    "import random\n",
    "import collections\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = collections.deque(maxlen = buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_list, a_list, r_list, s_next_list, done_list = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_next, done = transition\n",
    "            s_list.append(s)\n",
    "            a_list.append([a])\n",
    "            r_list.append([r])\n",
    "            s_next_list.append(s_next)\n",
    "            done_list.append([done])\n",
    "\n",
    "        return torch.tensor(s_list, dtype = torch.float, device = device), \\\n",
    "                torch.tensor(a_list, device = device), \\\n",
    "                torch.tensor(r_list, device = device), \\\n",
    "                torch.tensor(s_next_list, device = device), \\\n",
    "                torch.tensor(done_list, device = device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "memory = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "batch_size = 128\n",
    "gamma = 1.0\n",
    "\n",
    "def model_trainer(q_net, q_target_net, memory):\n",
    "    for i in range(10):\n",
    "        state, action, reward, s_next, done = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q_net(state)\n",
    "        q_a = q_out.gather(1, action)\n",
    "\n",
    "        max_q_a_next = q_target_net(s_next).max(1)[0].unsqueeze(1)\n",
    "        target = reward + gamma * max_q_a_next * done\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212.0\n",
      "2\n",
      "429.0\n",
      "3\n",
      "642.0\n",
      "4\n",
      "846.0\n",
      "5\n",
      "1051.0\n",
      "6\n",
      "1280.0\n",
      "7\n",
      "1474.0\n",
      "8\n",
      "1686.0\n",
      "9\n",
      "1908.0\n",
      "10\n",
      "2114.0\n",
      "11\n",
      "2329.0\n",
      "12\n",
      "2555.0\n",
      "13\n",
      "2766.0\n",
      "14\n",
      "2988.0\n",
      "15\n",
      "3195.0\n",
      "16\n",
      "3421.0\n",
      "17\n",
      "3626.0\n",
      "18\n",
      "3853.0\n",
      "19\n",
      "4058.0\n",
      "20\n",
      "4272.0\n",
      "21\n",
      "n_episode : 20, score : 214.2, n_buffer : 10000, eps : 7.9%\n",
      "0\n",
      "1\n",
      "218.0\n",
      "2\n",
      "436.0\n",
      "3\n",
      "667.0\n",
      "4\n",
      "891.0\n",
      "5\n",
      "1100.0\n",
      "6\n",
      "1315.0\n",
      "7\n",
      "1542.0\n",
      "8\n",
      "1759.0\n",
      "9\n",
      "1968.0\n",
      "10\n",
      "2190.0\n",
      "11\n",
      "2412.0\n",
      "12\n",
      "2634.0\n",
      "13\n",
      "2854.0\n",
      "14\n",
      "3090.0\n",
      "15\n",
      "3303.0\n",
      "16\n",
      "3513.0\n",
      "17\n",
      "3728.0\n",
      "18\n",
      "3948.0\n",
      "19\n",
      "4174.0\n",
      "20\n",
      "n_episode : 40, score : 220.2, n_buffer : 10000, eps : 7.8%\n",
      "0\n",
      "1\n",
      "221.0\n",
      "2\n",
      "415.0\n",
      "3\n",
      "615.0\n",
      "4\n",
      "822.0\n",
      "5\n",
      "1025.0\n",
      "6\n",
      "1235.0\n",
      "7\n",
      "1437.0\n",
      "8\n",
      "1660.0\n",
      "9\n",
      "1875.0\n",
      "10\n",
      "2089.0\n",
      "11\n",
      "2297.0\n",
      "12\n",
      "2512.0\n",
      "13\n",
      "2728.0\n",
      "14\n",
      "2937.0\n",
      "15\n",
      "3171.0\n",
      "16\n",
      "3391.0\n",
      "17\n",
      "3614.0\n",
      "18\n",
      "3846.0\n",
      "19\n",
      "4066.0\n",
      "20\n",
      "n_episode : 60, score : 215.25, n_buffer : 10000, eps : 7.7%\n",
      "0\n",
      "1\n",
      "214.0\n",
      "2\n",
      "454.0\n",
      "3\n",
      "661.0\n",
      "4\n",
      "875.0\n",
      "5\n",
      "1084.0\n",
      "6\n",
      "1306.0\n",
      "7\n",
      "1533.0\n",
      "8\n",
      "1753.0\n",
      "9\n",
      "1981.0\n",
      "10\n",
      "2221.0\n",
      "11\n",
      "2448.0\n",
      "12\n",
      "2665.0\n",
      "13\n",
      "2895.0\n",
      "14\n",
      "3106.0\n",
      "15\n",
      "3339.0\n",
      "16\n",
      "3566.0\n",
      "17\n",
      "3787.0\n",
      "18\n",
      "4000.0\n",
      "19\n",
      "4231.0\n",
      "20\n",
      "n_episode : 80, score : 223.0, n_buffer : 10000, eps : 7.6%\n",
      "0\n",
      "1\n",
      "238.0\n",
      "2\n",
      "435.0\n",
      "3\n",
      "672.0\n",
      "4\n",
      "883.0\n",
      "5\n",
      "1083.0\n",
      "6\n",
      "1297.0\n",
      "7\n",
      "1515.0\n",
      "8\n",
      "1719.0\n",
      "9\n",
      "1930.0\n",
      "10\n",
      "2145.0\n",
      "11\n",
      "2354.0\n",
      "12\n",
      "2561.0\n",
      "13\n",
      "2790.0\n",
      "14\n",
      "3022.0\n",
      "15\n",
      "3232.0\n",
      "16\n",
      "3442.0\n",
      "17\n",
      "3643.0\n",
      "18\n",
      "3857.0\n",
      "19\n",
      "4074.0\n",
      "20\n",
      "n_episode : 100, score : 213.7, n_buffer : 10000, eps : 7.5%\n",
      "0\n",
      "1\n",
      "216.0\n",
      "2\n",
      "413.0\n",
      "3\n",
      "614.0\n",
      "4\n",
      "815.0\n",
      "5\n",
      "1055.0\n",
      "6\n",
      "1285.0\n",
      "7\n",
      "1485.0\n",
      "8\n",
      "1707.0\n",
      "9\n",
      "1921.0\n",
      "10\n",
      "2116.0\n",
      "11\n",
      "2361.0\n",
      "12\n",
      "2569.0\n",
      "13\n",
      "2786.0\n",
      "14\n",
      "3018.0\n",
      "15\n",
      "3247.0\n",
      "16\n",
      "3463.0\n",
      "17\n",
      "3693.0\n",
      "18\n",
      "3917.0\n",
      "19\n",
      "4141.0\n",
      "20\n",
      "n_episode : 120, score : 217.9, n_buffer : 10000, eps : 7.4%\n",
      "0\n",
      "1\n",
      "231.0\n",
      "2\n",
      "412.0\n",
      "3\n",
      "590.0\n",
      "4\n",
      "827.0\n",
      "5\n",
      "1034.0\n",
      "6\n",
      "1233.0\n",
      "7\n",
      "1445.0\n",
      "8\n",
      "1652.0\n",
      "9\n",
      "1856.0\n",
      "10\n",
      "2050.0\n",
      "11\n",
      "2245.0\n",
      "12\n",
      "2450.0\n",
      "13\n",
      "2657.0\n",
      "14\n",
      "2866.0\n",
      "15\n",
      "3068.0\n",
      "16\n",
      "3277.0\n",
      "17\n",
      "3482.0\n",
      "18\n",
      "3695.0\n",
      "19\n",
      "3896.0\n",
      "20\n",
      "n_episode : 140, score : 205.15, n_buffer : 10000, eps : 7.3%\n",
      "0\n",
      "1\n",
      "208.0\n",
      "2\n",
      "467.0\n",
      "3\n",
      "654.0\n",
      "4\n",
      "875.0\n",
      "5\n",
      "1087.0\n",
      "6\n",
      "1297.0\n",
      "7\n",
      "1520.0\n",
      "8\n",
      "1729.0\n",
      "9\n",
      "1939.0\n",
      "10\n",
      "2160.0\n",
      "11\n",
      "2372.0\n",
      "12\n",
      "2582.0\n",
      "13\n",
      "2796.0\n",
      "14\n",
      "3024.0\n",
      "15\n",
      "3248.0\n",
      "16\n",
      "3470.0\n",
      "17\n",
      "3686.0\n",
      "18\n",
      "3925.0\n",
      "19\n",
      "4154.0\n",
      "20\n",
      "n_episode : 160, score : 218.5, n_buffer : 10000, eps : 7.2%\n",
      "0\n",
      "1\n",
      "208.0\n",
      "2\n",
      "430.0\n",
      "3\n",
      "641.0\n",
      "4\n",
      "857.0\n",
      "5\n",
      "1070.0\n",
      "6\n",
      "1321.0\n",
      "7\n",
      "1548.0\n",
      "8\n",
      "1769.0\n",
      "9\n",
      "2009.0\n",
      "10\n",
      "2225.0\n",
      "11\n",
      "2449.0\n",
      "12\n",
      "2692.0\n",
      "13\n",
      "2935.0\n",
      "14\n",
      "3176.0\n",
      "15\n",
      "3430.0\n",
      "16\n",
      "3672.0\n",
      "17\n",
      "3894.0\n",
      "18\n",
      "4129.0\n",
      "19\n",
      "4385.0\n",
      "20\n",
      "n_episode : 180, score : 231.1, n_buffer : 10000, eps : 7.1%\n",
      "0\n",
      "1\n",
      "238.0\n",
      "2\n",
      "465.0\n",
      "3\n",
      "718.0\n",
      "4\n",
      "971.0\n",
      "5\n",
      "1210.0\n",
      "6\n",
      "1478.0\n",
      "7\n",
      "1719.0\n",
      "8\n",
      "1949.0\n",
      "9\n",
      "2217.0\n",
      "10\n",
      "2462.0\n",
      "11\n",
      "2728.0\n",
      "12\n",
      "2955.0\n",
      "13\n",
      "3211.0\n",
      "14\n",
      "3471.0\n",
      "15\n",
      "3726.0\n",
      "16\n",
      "3962.0\n",
      "17\n",
      "4221.0\n",
      "18\n",
      "4461.0\n",
      "19\n",
      "4722.0\n",
      "20\n",
      "n_episode : 200, score : 248.45, n_buffer : 10000, eps : 7.0%\n",
      "0\n",
      "1\n",
      "253.0\n",
      "2\n",
      "459.0\n",
      "3\n",
      "703.0\n",
      "4\n",
      "1002.0\n",
      "5\n",
      "1241.0\n",
      "6\n",
      "1508.0\n",
      "7\n",
      "1770.0\n",
      "8\n",
      "2024.0\n",
      "9\n",
      "2260.0\n",
      "10\n",
      "2522.0\n",
      "11\n",
      "2804.0\n",
      "12\n",
      "3053.0\n",
      "13\n",
      "3335.0\n",
      "14\n",
      "3581.0\n",
      "15\n",
      "3855.0\n",
      "16\n",
      "4118.0\n",
      "17\n",
      "4372.0\n",
      "18\n",
      "4616.0\n",
      "19\n",
      "4860.0\n",
      "20\n",
      "n_episode : 220, score : 256.85, n_buffer : 10000, eps : 6.9%\n",
      "0\n",
      "1\n",
      "269.0\n",
      "2\n",
      "483.0\n",
      "3\n",
      "731.0\n",
      "4\n",
      "996.0\n",
      "5\n",
      "1242.0\n",
      "6\n",
      "1495.0\n",
      "7\n",
      "1771.0\n",
      "8\n",
      "2027.0\n",
      "9\n",
      "2299.0\n",
      "10\n",
      "2564.0\n",
      "11\n",
      "2829.0\n",
      "12\n",
      "3064.0\n",
      "13\n",
      "3334.0\n",
      "14\n",
      "3587.0\n",
      "15\n",
      "3861.0\n",
      "16\n",
      "4113.0\n",
      "17\n",
      "4365.0\n",
      "18\n",
      "4635.0\n",
      "19\n",
      "4897.0\n",
      "20\n",
      "n_episode : 240, score : 256.6, n_buffer : 10000, eps : 6.8%\n",
      "0\n",
      "1\n",
      "268.0\n",
      "2\n",
      "501.0\n",
      "3\n",
      "815.0\n",
      "4\n",
      "1110.0\n",
      "5\n",
      "1384.0\n",
      "6\n",
      "1665.0\n",
      "7\n",
      "1951.0\n",
      "8\n",
      "2237.0\n",
      "9\n",
      "2504.0\n",
      "10\n",
      "2796.0\n",
      "11\n",
      "3099.0\n",
      "12\n",
      "3387.0\n",
      "13\n",
      "3688.0\n",
      "14\n",
      "3974.0\n",
      "15\n",
      "4257.0\n",
      "16\n",
      "4535.0\n",
      "17\n",
      "4841.0\n",
      "18\n",
      "5130.0\n",
      "19\n",
      "5406.0\n",
      "20\n",
      "n_episode : 260, score : 285.75, n_buffer : 10000, eps : 6.7%\n",
      "0\n",
      "1\n",
      "282.0\n",
      "2\n",
      "538.0\n",
      "3\n",
      "797.0\n",
      "4\n",
      "1146.0\n",
      "5\n",
      "1399.0\n",
      "6\n",
      "1704.0\n",
      "7\n",
      "1976.0\n",
      "8\n",
      "2265.0\n",
      "9\n",
      "2582.0\n",
      "10\n",
      "2868.0\n",
      "11\n",
      "3147.0\n",
      "12\n",
      "3448.0\n",
      "13\n",
      "3758.0\n",
      "14\n",
      "3821.0\n",
      "15\n",
      "4113.0\n",
      "16\n",
      "4418.0\n",
      "17\n",
      "4440.0\n",
      "18\n",
      "4722.0\n",
      "19\n",
      "5040.0\n",
      "20\n",
      "n_episode : 280, score : 267.95, n_buffer : 10000, eps : 6.6%\n",
      "0\n",
      "1\n",
      "270.0\n",
      "2\n",
      "1271.0\n",
      "3\n",
      "1538.0\n",
      "4\n",
      "1885.0\n",
      "5\n",
      "1904.0\n",
      "6\n",
      "2284.0\n",
      "7\n",
      "2563.0\n",
      "8\n",
      "2922.0\n",
      "9\n",
      "3286.0\n",
      "10\n",
      "3619.0\n",
      "11\n",
      "3957.0\n",
      "12\n",
      "4270.0\n",
      "13\n",
      "4595.0\n",
      "14\n",
      "4878.0\n",
      "15\n",
      "5239.0\n",
      "16\n",
      "5549.0\n",
      "17\n",
      "5852.0\n",
      "18\n",
      "6159.0\n",
      "19\n",
      "6491.0\n",
      "20\n",
      "n_episode : 300, score : 341.85, n_buffer : 10000, eps : 6.5%\n",
      "0\n",
      "1\n",
      "281.0\n",
      "2\n",
      "699.0\n",
      "3\n",
      "1156.0\n",
      "4\n",
      "1630.0\n",
      "5\n",
      "2132.0\n",
      "6\n",
      "2497.0\n",
      "7\n",
      "2833.0\n",
      "8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[84], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m     14\u001b[0m     a \u001b[38;5;241m=\u001b[39m q_net\u001b[38;5;241m.\u001b[39msample_action(torch\u001b[38;5;241m.\u001b[39mfrom_numpy(s)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device), epsilon)\n\u001b[1;32m---> 16\u001b[0m     s_next, reward, done, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(a)\n\u001b[0;32m     17\u001b[0m     done_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m done \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     19\u001b[0m     memory\u001b[38;5;241m.\u001b[39mput((s, a, reward, s_next, done_mask))\n",
      "File \u001b[1;32mc:\\Users\\Noh\\anaconda3\\Lib\\site-packages\\gymnasium\\wrappers\\common.py:112\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps \u001b[38;5;241m=\u001b[39m max_episode_steps\n\u001b[0;32m    110\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\n\u001b[0;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[0;32m    114\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[0;32m    115\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \n\u001b[0;32m    117\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    123\u001b[0m \n\u001b[0;32m    124\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    125\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "step_count = 1\n",
    "score = 0\n",
    "print_interval = 20\n",
    "\n",
    "for n_epi in range(1500):\n",
    "    epsilon = max(0.01, 0.08 - 0.01 * (n_epi / 200))\n",
    "    s, info = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = q_net.sample_action(torch.from_numpy(s).float().to(device), epsilon)\n",
    "\n",
    "        s_next, reward, done, truncated, info = env.step(a)\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "\n",
    "        memory.put((s, a, reward, s_next, done_mask))\n",
    "        s = s_next\n",
    "        score += reward\n",
    "\n",
    "        if (score / step_count > 1000):\n",
    "            break\n",
    "\n",
    "        if (done):\n",
    "            break\n",
    "\n",
    "    step_count += 1\n",
    "    if (memory.size() > 2000):\n",
    "        model_trainer(q_net, q_target_net, memory)\n",
    "\n",
    "    if (n_epi % print_interval == 0) and (n_epi > 0):\n",
    "        q_target_net.load_state_dict(q_net.state_dict())\n",
    "        print(\"n_episode : {}, score : {}, n_buffer : {}, eps : {:.1f}%\".format(n_epi, score / print_interval, memory.size(), epsilon * 100))\n",
    "\n",
    "        step_count = 1\n",
    "        score = 0\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "\n",
    "import time\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype = torch.float32, device = device).unsqueeze(0)\n",
    "\n",
    "for i in range(500):\n",
    "    action = q_net.sample_action(state, epsilon)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    state = next_state\n",
    "    state = torch.tensor(state, dtype = torch.float32, device = device).unsqueeze(0)\n",
    "    if (terminated):\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype = torch.float32, device = device).unsqueeze(0)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
