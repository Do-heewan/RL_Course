{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gymnasium\n",
      "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\noh\\anaconda3\\lib\\site-packages (from gymnasium) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in c:\\users\\noh\\anaconda3\\lib\\site-packages (from gymnasium) (2.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in c:\\users\\noh\\anaconda3\\lib\\site-packages (from gymnasium) (4.9.0)\n",
      "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
      "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
      "   ---------------------------------------- 0.0/958.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/958.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 10.2/958.1 kB ? eta -:--:--\n",
      "   - ------------------------------------- 41.0/958.1 kB 495.5 kB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 133.1/958.1 kB 1.1 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 450.6/958.1 kB 2.8 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 839.7/958.1 kB 4.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  952.3/958.1 kB 4.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 958.1/958.1 kB 3.4 MB/s eta 0:00:00\n",
      "Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Installing collected packages: farama-notifications, gymnasium\n",
      "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. make\n",
    "import gymnasium as gym\n",
    "env = gym.make('CartPole-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. reset\n",
    "observation, info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. step\n",
    "observation, reward, terminated, truncated, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. render(model)\n",
    "\n",
    "import gymnasium as gym\n",
    "import time\n",
    "\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "state, info = env.reset()\n",
    "\n",
    "for i in range(100):\n",
    "    action = 0\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "    time.sleep(0.01)\n",
    "    state = next_state\n",
    "\n",
    "    if (terminated):\n",
    "        state, info = env.reset()\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DQN Model\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(4, 128)\n",
    "        self.layer2 = nn.Linear(128, 128)\n",
    "        self.layer3 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "\n",
    "        return self.layer3(x)\n",
    "\n",
    "    def sample_action(self, state, eps):\n",
    "        out = self.forward(state)\n",
    "        coin = random.random()\n",
    "        if (coin < eps):\n",
    "            return random.randint(0, 1)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device : cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"device : {device}\")\n",
    "\n",
    "learning_rate = 0.0005\n",
    "\n",
    "q_net = DQN().to(device)\n",
    "q_target_net = DQN().to(device)\n",
    "q_target_net.load_state_dict(q_net.state_dict())\n",
    "optimizer = optim.Adam(q_net.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay Buffer\n",
    "\n",
    "import random\n",
    "import collections\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self, buffer_limit):\n",
    "        self.buffer = collections.deque(maxlen = buffer_limit)\n",
    "\n",
    "    def put(self, transition):\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n):\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_list, a_list, r_list, s_next_list, done_list = [], [], [], [], []\n",
    "\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_next, done = transition\n",
    "            s_list.append(s)\n",
    "            a_list.append([a])\n",
    "            r_list.append([r])\n",
    "            s_next_list.append(s_next)\n",
    "            done_list.append([done])\n",
    "\n",
    "        return torch.tensor(s_list, dtype = torch.float, device = device), \\\n",
    "                torch.tensor(a_list, device = device), \\\n",
    "                torch.tensor(r_list, device = device), \\\n",
    "                torch.tensor(s_next_list, device = device), \\\n",
    "                torch.tensor(done_list, device = device)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "memory = ReplayBuffer(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model\n",
    "\n",
    "batch_size = 128\n",
    "gamma = 1.0\n",
    "\n",
    "def model_trainer(q_net, q_target_net, memory):\n",
    "    for i in range(10):\n",
    "        state, action, reward, s_next, done = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q_net(state)\n",
    "        q_a = q_out.gather(1, action)\n",
    "\n",
    "        max_q_a_next = q_target_net(s_next).max(1)[0].unsqueeze(1)\n",
    "        target = reward + gamma * max_q_a_next * done\n",
    "\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 20, score : 41.5, n_buffer : 830, eps : 7.9%\n",
      "n_episode : 40, score : 33.8, n_buffer : 1506, eps : 7.8%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Noh\\AppData\\Local\\Temp\\ipykernel_13772\\3885697836.py:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ..\\torch\\csrc\\utils\\tensor_new.cpp:277.)\n",
      "  return torch.tensor(s_list, dtype = torch.float, device = device), \\\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_episode : 60, score : 38.5, n_buffer : 2276, eps : 7.7%\n",
      "n_episode : 80, score : 24.75, n_buffer : 2771, eps : 7.6%\n",
      "n_episode : 100, score : 20.45, n_buffer : 3180, eps : 7.5%\n",
      "n_episode : 120, score : 33.9, n_buffer : 3858, eps : 7.4%\n",
      "n_episode : 140, score : 45.0, n_buffer : 4758, eps : 7.3%\n",
      "n_episode : 160, score : 55.6, n_buffer : 5870, eps : 7.2%\n",
      "n_episode : 180, score : 221.0, n_buffer : 10000, eps : 7.1%\n",
      "n_episode : 200, score : 243.85, n_buffer : 10000, eps : 7.0%\n",
      "n_episode : 220, score : 243.15, n_buffer : 10000, eps : 6.9%\n",
      "n_episode : 240, score : 238.95, n_buffer : 10000, eps : 6.8%\n",
      "n_episode : 260, score : 248.15, n_buffer : 10000, eps : 6.7%\n",
      "n_episode : 280, score : 233.75, n_buffer : 10000, eps : 6.6%\n",
      "n_episode : 300, score : 300.55, n_buffer : 10000, eps : 6.5%\n",
      "n_episode : 320, score : 248.65, n_buffer : 10000, eps : 6.4%\n",
      "n_episode : 340, score : 228.75, n_buffer : 10000, eps : 6.3%\n",
      "n_episode : 360, score : 187.85, n_buffer : 10000, eps : 6.2%\n",
      "n_episode : 380, score : 224.6, n_buffer : 10000, eps : 6.1%\n",
      "n_episode : 400, score : 228.4, n_buffer : 10000, eps : 6.0%\n",
      "n_episode : 420, score : 211.0, n_buffer : 10000, eps : 5.9%\n",
      "n_episode : 440, score : 222.65, n_buffer : 10000, eps : 5.8%\n",
      "n_episode : 460, score : 204.8, n_buffer : 10000, eps : 5.7%\n",
      "n_episode : 480, score : 187.45, n_buffer : 10000, eps : 5.6%\n",
      "n_episode : 500, score : 182.05, n_buffer : 10000, eps : 5.5%\n",
      "n_episode : 520, score : 160.2, n_buffer : 10000, eps : 5.4%\n",
      "n_episode : 540, score : 159.65, n_buffer : 10000, eps : 5.3%\n",
      "n_episode : 560, score : 119.35, n_buffer : 10000, eps : 5.2%\n",
      "n_episode : 580, score : 36.0, n_buffer : 10000, eps : 5.1%\n",
      "n_episode : 600, score : 30.25, n_buffer : 10000, eps : 5.0%\n",
      "n_episode : 620, score : 30.9, n_buffer : 10000, eps : 4.9%\n",
      "n_episode : 640, score : 31.75, n_buffer : 10000, eps : 4.8%\n",
      "n_episode : 660, score : 40.25, n_buffer : 10000, eps : 4.7%\n",
      "n_episode : 680, score : 142.85, n_buffer : 10000, eps : 4.6%\n",
      "n_episode : 700, score : 147.2, n_buffer : 10000, eps : 4.5%\n",
      "n_episode : 720, score : 149.8, n_buffer : 10000, eps : 4.4%\n",
      "n_episode : 740, score : 158.35, n_buffer : 10000, eps : 4.3%\n",
      "n_episode : 760, score : 147.6, n_buffer : 10000, eps : 4.2%\n",
      "n_episode : 780, score : 144.25, n_buffer : 10000, eps : 4.1%\n",
      "n_episode : 800, score : 155.0, n_buffer : 10000, eps : 4.0%\n",
      "n_episode : 820, score : 148.65, n_buffer : 10000, eps : 3.9%\n",
      "n_episode : 840, score : 150.35, n_buffer : 10000, eps : 3.8%\n",
      "n_episode : 860, score : 142.55, n_buffer : 10000, eps : 3.7%\n",
      "n_episode : 880, score : 143.6, n_buffer : 10000, eps : 3.6%\n",
      "n_episode : 900, score : 135.8, n_buffer : 10000, eps : 3.5%\n",
      "n_episode : 920, score : 139.05, n_buffer : 10000, eps : 3.4%\n",
      "n_episode : 940, score : 138.6, n_buffer : 10000, eps : 3.3%\n",
      "n_episode : 960, score : 136.85, n_buffer : 10000, eps : 3.2%\n",
      "n_episode : 980, score : 135.55, n_buffer : 10000, eps : 3.1%\n",
      "n_episode : 1000, score : 132.8, n_buffer : 10000, eps : 3.0%\n",
      "n_episode : 1020, score : 132.35, n_buffer : 10000, eps : 2.9%\n",
      "n_episode : 1040, score : 132.35, n_buffer : 10000, eps : 2.8%\n",
      "n_episode : 1060, score : 130.95, n_buffer : 10000, eps : 2.7%\n",
      "n_episode : 1080, score : 127.75, n_buffer : 10000, eps : 2.6%\n",
      "n_episode : 1100, score : 122.95, n_buffer : 10000, eps : 2.5%\n",
      "n_episode : 1120, score : 124.1, n_buffer : 10000, eps : 2.4%\n",
      "n_episode : 1140, score : 126.35, n_buffer : 10000, eps : 2.3%\n",
      "n_episode : 1160, score : 129.6, n_buffer : 10000, eps : 2.2%\n",
      "n_episode : 1180, score : 119.25, n_buffer : 10000, eps : 2.1%\n",
      "n_episode : 1200, score : 122.15, n_buffer : 10000, eps : 2.0%\n",
      "n_episode : 1220, score : 115.7, n_buffer : 10000, eps : 1.9%\n",
      "n_episode : 1240, score : 124.6, n_buffer : 10000, eps : 1.8%\n",
      "n_episode : 1260, score : 120.0, n_buffer : 10000, eps : 1.7%\n",
      "n_episode : 1280, score : 126.75, n_buffer : 10000, eps : 1.6%\n",
      "n_episode : 1300, score : 123.85, n_buffer : 10000, eps : 1.5%\n",
      "n_episode : 1320, score : 127.8, n_buffer : 10000, eps : 1.4%\n",
      "n_episode : 1340, score : 124.6, n_buffer : 10000, eps : 1.3%\n",
      "n_episode : 1360, score : 127.75, n_buffer : 10000, eps : 1.2%\n",
      "n_episode : 1380, score : 120.8, n_buffer : 10000, eps : 1.1%\n",
      "n_episode : 1400, score : 107.35, n_buffer : 10000, eps : 1.0%\n",
      "n_episode : 1420, score : 113.75, n_buffer : 10000, eps : 1.0%\n",
      "n_episode : 1440, score : 118.6, n_buffer : 10000, eps : 1.0%\n",
      "n_episode : 1460, score : 122.3, n_buffer : 10000, eps : 1.0%\n",
      "n_episode : 1480, score : 120.3, n_buffer : 10000, eps : 1.0%\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "step_count = 1\n",
    "score = 0\n",
    "print_interval = 20\n",
    "\n",
    "for n_epi in range(1500):\n",
    "    epsilon = max(0.01, 0.08 - 0.01 * (n_epi / 200))\n",
    "    s, info = env.reset()\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = q_net.sample_action(torch.from_numpy(s).float().to(device), epsilon)\n",
    "\n",
    "        s_next, reward, done, truncated, info = env.step(a)\n",
    "        done_mask = 0.0 if done else 1.0\n",
    "\n",
    "        memory.put((s, a, reward, s_next, done_mask))\n",
    "        s = s_next\n",
    "        score += reward\n",
    "\n",
    "        if (score / step_count > 1000):\n",
    "            break\n",
    "\n",
    "        if (done):\n",
    "            break\n",
    "\n",
    "    step_count += 1\n",
    "    if (memory.size() > 2000):\n",
    "        model_trainer(q_net, q_target_net, memory)\n",
    "\n",
    "    if (n_epi % print_interval == 0) and (n_epi > 0):\n",
    "        q_target_net.load_state_dict(q_net.state_dict())\n",
    "        print(\"n_episode : {}, score : {}, n_buffer : {}, eps : {:.1f}%\".format(n_epi, score / print_interval, memory.size(), epsilon * 100))\n",
    "\n",
    "        step_count = 1\n",
    "        score = 0\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result\n",
    "\n",
    "import time\n",
    "env = gym.make(\"CartPole-v1\", render_mode = \"human\")\n",
    "state, info = env.reset()\n",
    "state = torch.tensor(state, dtype = torch.float32, device = device).unsqueeze(0)\n",
    "\n",
    "for i in range(500):\n",
    "    action = q_net.sample_action(state, epsilon)\n",
    "    next_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    time.sleep(0.01)\n",
    "\n",
    "    state = next_state\n",
    "    state = torch.tensor(state, dtype = torch.float32, device = device).unsqueeze(0)\n",
    "    if (terminated):\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype = torch.float32, device = device).unsqueeze(0)\n",
    "\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
